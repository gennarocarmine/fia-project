\documentclass[12pt]{article}

% --- Lingua e Codifica ---
\usepackage[italian,english]{babel}

% --- Matematica ---
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{bm}

% --- Colori e Grafica ---
\usepackage[table,xcdraw,svgnames]{xcolor} 
\usepackage{graphicx} % Required for inserting images & resizing

% --- Tabelle ---
\usepackage{array}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{adjustbox}

% --- Formattazione e Layout ---
\usepackage[italian]{minitoc}
\usepackage{fancybox}
\usepackage{fancyhdr}
\usepackage{lscape}
\usepackage{placeins}
\usepackage{float}
\usepackage{caption}
\usepackage{soul}

% --- Utilità e Codice ---
\usepackage{verbatim}
\usepackage{url}
\usepackage{listings}
\usepackage{makeidx}
\usepackage{comment}
\usepackage{animate}

% --- Bibliografia ---
\usepackage{biblatex}
\addbibresource{sample.bib}

% --- Collegamenti (Caricare per ultimo) ---

%\titleformat{\chapter}{\normalfont\huge}{\textbf\thechapter.}{20pt}%{\huge\textbf}

%inizio documento
\begin{document}
\selectlanguage{italian}

%inizio copertina
\begin{titlepage}
\begin{center}
    \begin{figure}
        \includegraphics[width=3.0cm, height=3.0cm]{images/unisa.png}
        \centering
    \end{figure}
    {\Large Università degli Studi di Salerno}\\[0.2truecm]
    {\large Dipartimento di Informatica\\Corso di Laurea Triennale in Informatica}\\
    \hrulefill
    \vfill
    {\large Fondamenti di Intelligenza Artificiale (FIA)}\\[0.2truecm]
    %{\large Project Proposal}\\[0.2truecm]
    %{\Large Informatica}\\
    \vfill
    {\Huge {\bf T.W.A.I.}}\\
    \vspace{0.3cm}
    {\Large \textit{(Tris Was Already Invented)}}\\
    
    \vfill\vfill
    
    
    {\bf Docente} \hfill {\bf Studenti}\ \hfill  {\bf Matricola}\  \\
    Prof.  Fabio Palomba \hfill Tozza Gennaro Carmine \hfill 0512120382 \\
    \hfill \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Valva Lorenzo \hfill 0512119639 \\
    \vfill
    [\url{https://github.com/gennarocarmine/fia-project.git}]
    \vfill
    \hrulefill 
    \begin{center} Anno Accademico 2025-2026 \end{center}
    
\end{center}
\end{titlepage}
%fine copertina

\tableofcontents
\newpage

\begin{abstract}
\noindent
Il presente elaborato documenta lo sviluppo di T.W.A.I. (Tris Was Already Invented), un sistema di Intelligenza Artificiale applicato al gioco Forza 4. L'obiettivo del progetto è confrontare due paradigmi fondamentali dell'IA: l'approccio simbolico, implementato tramite l'algoritmo Minimax con ottimizzazioni Alpha-Beta Pruning e Move Ordering, e l'approccio connessionista, realizzato mediante una Rete Neurale MLP (Multi-Layer Perceptron).
Particolare rilievo assume la fase di \textit{Data Engineering}, in cui l'agente simbolico è stato impiegato come Oracolo per la generazione e l'etichettatura automatica di un dataset sintetico di oltre 100.000 configurazioni.
I risultati sperimentali evidenziano il trade-off tra i due modelli: mentre l'approccio neurale offre un tempo di inferenza costante ($O(1)$) con un'accuratezza dell'86\%, l'algoritmo di ricerca garantisce la correttezza formale e, grazie alle ottimizzazioni introdotte, raggiunge prestazioni \textit{real-time} (0.13s) a profondità competitive, pur rimanendo vincolato alla complessità esponenziale dello spazio degli stati.
\end{abstract}

\newpage

\section{Introduzione}
Il periodo festivo porta con sé tradizioni imprescindibili: i panettoni, le calze e le discussioni con i parenti che, pur lamentandosi dell'invadenza dell'Intelligenza Artificiale, passano il tempo a guardare video di gattini che ballano sui social. Ma porta anche le immancabili giocate a carte. 
È stato proprio durante l'ennesima partita persa a \textit{Sette e Mezzo} che è sorta la domanda fatidica: ``E se al nostro posto giocasse un'IA?''.\\\\
\noindent
L'idea embrionale del progetto, inizialmente battezzata con il nome in codice \textbf{(IA)'M FINE} (un gioco di parole sulla dichiarazione ``sto bene'' usata per non ricevere altre carte), mirava a creare un agente imbattibile in questo gioco tradizionale.\\\\
\noindent
Tuttavia, durante la fase di analisi preliminare, è emersa una criticità strutturale. A differenza di giochi come il Blackjack, dove il banco espone parzialmente il proprio stato (una carta scoperta e una coperta), nel Sette e Mezzo l'interazione inizia con un livello di informazione imperfetta molto marcato, dato che la carta iniziale è privata e coperta.
Questa caratteristica sposta il problema dal ragionamento strategico puro alla gestione della probabilità e del rischio (stocasticità), rendendo difficile un confronto diretto tra l'efficacia di algoritmi deterministici come il Minimax.

\subsubsection*{Cambio di Rotta}
Cercando un'alternativa deterministica (a informazione perfetta), il pensiero è corso subito al gioco più classico: il Tris (Tic-Tac-Toe). Ma anche qui sorgeva un problema opposto: il Tris è un gioco banale, con uno spazio degli stati minuscolo e una strategia ottimale che porta inevitabilmente al pareggio.\\\
\noindent
Da qui nasce il nome definitivo del progetto: \textbf{T.W.A.I.} (\textit{Tris Was Already Invented}).\\\\
\noindent
Abbiamo quindi scelto il ``fratello maggiore'' del Tris: \textbf{Forza 4} (Connect-4). In questo contesto, l'intero stato del gioco è visibile a entrambi i contendenti, eliminando il fattore fortuna e permettendo di concentrare l'analisi esclusivamente sulle capacità computazionali e strategiche dell'Intelligenza Artificiale.

\section{Definizione del problema} 
Connect Four (noto anche come Connect 4, Four Up, Plot Four, Find Four, Captain's Mistress, Four in a Row, Drop Four, e in Unione Sovietica, Gravitrips) è un gioco in cui i giocatori scelgono un colore e poi si alternano lasciando cadere gettoni colorati in una griglia a sei file, sette colonne verticalmente sospesa. \\
I pezzi cadono direttamente, occupando lo spazio più basso disponibile all'interno della colonna. L'obiettivo del gioco è quello di essere il primo a formare una linea orizzontale, verticale o diagonale di quattro dei propri gettoni. \cite{5}

\begin{figure}[H]
    \centering
        \includegraphics[width=0.4\textwidth]{images/connect_four.png}
    \caption{Immagine del gioco Forza 4.}
\end{figure}
\noindent
Il gioco rientra nella categoria dei giochi a somma zero, a informazione perfetta e deterministici. Sebbene le regole siano semplici, la complessità computazionale non è trascurabile.\\\\
\noindent
Nonostante il gioco sia stato "risolto" matematicamente (il primo giocatore ha una strategia vincente se gioca perfettamente), per un agente limitato da risorse computazionali e tempo di risposta (real-time interaction), la ricerca esaustiva è impraticabile.
Il problema richiede quindi l'utilizzo di euristiche o approssimatori di funzione per valutare la bontà di una mossa senza dover esplorare l'intero albero di gioco fino alle foglie.

\subsection{Obiettivi} 
L'obiettivo principale del progetto è sviluppare e confrontare due diversi approcci per la risoluzione del gioco Forza 4: uno basato sulla teoria della ricerca nello spazio degli stati e uno basato sull'apprendimento automatico supervisionato. Il sistema non utilizzerà dataset precostruiti, ma genererà autonomamente i dati necesaari per l'addestramento. Nello specifico, gli obiettivi sono:
\begin{itemize}
    \item \textbf{Generare un Dataset:} Creare un dataset di training personalizzato che superi i limiti delle simulazioni puramente casuali (Random vs Random). Il dataset verrà generato registrando partite miste:
     \begin{itemize}
        \item Bot Random vs Bot Random: Per esplorare lo spazio degli stati in modo ampio.
        \item Minimax vs Random: Per insegnare alla rete neurale pattern strategici, mosse di blocco e sequenze vincenti, garantendo dati di qualità superiore per l'addestramento.
     \end{itemize}
     L'obiettivo è ottenere un dataset ricco e variegato che consenta alla rete neurale di apprendere strategie efficaci.
    \item \textbf{Implementare la pipeline di Ricerca(Minimax):} Sviluppare un agente basato sull'algoritmo Minimax con Alpha-Beta Pruning. Questo modulo avrà il duplice scopo di:
     \begin{itemize}
        \item Fornire un avversario algoritmico "forte" in grado di calcolare la mossa ottimale esplorando l'albero di gioco.
        \item Agire come "oracolo" per la generazione dei dati di training.
     \end{itemize}
    \item \textbf{Implementare la pipeline di Apprendimento (MLP):} Sviluppare e addestrare una rete neurale (MLP) utilizzando il dataset generato. L'obiettivo è ottenere un modello in grado di predire la mossa migliore istantaneamente (approssimando la funzione di valutazione), imitando la logica del Minimax ma con tempi di risposta drasticamente inferiori.
    \item \textbf{Sviluppare un'interfaccia interattiva:} Creare un interfaccia grafica che permetta all'utente di sfidare entrambe le IA (Minmax e MLP).
\end{itemize}

\subsection{Specifica PEAS} 
La descrizione formale dell'ambiente operativo dell'agente è definita secondo il modello PEAS (\textit{Performance, Environment, Actuators, Sensors}):

\begin{itemize}
    \item \textbf{Performance (Misure di Prestazione):}
    \begin{itemize}
        \item Efficienza: Numero di mosse minimo per raggiungere la vittoria.
        \item Correttezza: Evitare mosse non valide (es. inserire in una colonna piena).
        \item Velocità di risposta: Tempo impiegato per calcolare la mossa.
        \item Tasso di vittoria: Percentuale di partite vinte contro vari tipi di avversari.
    \end{itemize}
    
    \item \textbf{Environment (Ambiente):}
    \begin{itemize}
        \item Griglia di gioco: Una matrice 6 X 7.
        \item Avversario: Un essere umano o un'altra IA.
    \end{itemize}
    
    \item \textbf{Actuators (Attuatori/Azioni):}
    \begin{itemize}
        \item Inserimento gettone: l'attuatore sceglie una delle 7 colonne disponibili.
        \item Segnalazione: comunicazione della mossa o dichiarazione di vittoria/resa.
    \end{itemize}
    
    \item \textbf{Sensors (Sensori/Percezioni):}
    \begin{itemize}
        \item Lettore di matrice: Funzione che scansiona lo sttao attuale della scacchiera per sapere dove sono i propri gettoni, quelli dell'avversario e gli spazi vuoti.
        \item Rilevatore di turno: Funziona che indica alla'gente quando è il suo momento di agire.
    \end{itemize}
\end{itemize}

\subsubsection{Caratteristiche dell’ambiente} 
L'ambiente di gioco Forza 4 presenta le seguenti proprietà:
\begin{itemize}
    \item \textbf{Completamente Osservabile:} L'agente vede l'intera griglia di gioco, non ci osno informazioni nascoste.
    \item \textbf{Deterministico:} Lo stato successivo dell'ambiente è completamente determinato dallo stato corrente e dall'azione eseuita dall'agente.
    \item \textbf{Discreto:} L'ambiente fornisce un numero limitato di percezioni e azioni distinte, chiaramente definite.
\end{itemize}

\subsection{Analisi del problema} 
Lo spazio degli stati teorico è costituito da tutte le possibili configurazioni di una griglia $6 \times 7$ con tre possibili valori per cella (Vuoto, Giocatore 1, Giocatore 2), portando a un limite superiore di $3^{42}$ stati. Tuttavia, considerando i vincoli di gravità (le pedine devono poggiare su altre pedine o sul fondo), il numero reale di stati legali è stimato intorno a $4.5 \times 10^{12}$.

\section{Tecnologie e Strumenti Utilizzati}
Lo sviluppo di T.W.A.I. è stato realizzato interamente in linguaggio Python. La scelta di questo linguaggio è stata dettata dalla vastità del suo ecosistema per il calcolo scientifico e l'intelligenza artificiale.
Di seguito vengono descritte le librerie principali che compongono l'architettura del sistema:

\begin{itemize}
    \item \textbf{numpy:} Utilizzata per la rappresentazione in memoria della griglia di gioco. La scacchiera $6 \times 7$ è gestita come un array bidimensionale (\texttt{ndarray}) di interi. L'efficienza di NumPy nelle operazioni vettoriali e nella manipolazione di matrici è fondamentale per velocizzare le funzioni di valutazione e controllo della vittoria.
    \item \textbf{pandas:} Impiegata nel modulo di Data Engineering per la gestione strutturata del dataset. 
\end{itemize}

\begin{itemize}
    \item \textbf{scikit-learn:} La libreria di riferimento per l'implementazione della pipeline di apprendimento. Nello specifico, sono stati utilizzati:
    \begin{itemize}
        \item \texttt{MLPClassifier}: Per la creazione e l'addestramento della rete neurale Multi-Layer Perceptron.
        \item \texttt{train\_test\_split}: Per la partizione corretta del dataset in Training Set e Test Set.
        \item \texttt{metrics}: Per il calcolo di accuratezza, matrici di confusione e report di classificazione.
    \end{itemize}
    \item \textbf{matplotlib e seaborn:} Utilizzate nella fase di analisi per generare grafici e visualizzazioni (curve di loss, heatmap della matrice di confusione) utili a monitorare le performance dell'agente.
\end{itemize}
\noindent
Per garantire un'interazione fluida e accessibile con gli agenti intelligenti, è stata sviluppata un'interfaccia web utilizzando \textbf{Streamlit}, un framework open-source orientato al Data Science.\\\\
\noindent
La scelta di Streamlit ha permesso di trasformare gli script di backend (logica di gioco e modelli AI) in un'applicazione web interattiva in tempi ridotti, senza la necessità di gestire complessi stack front-end (HTML/CSS/JS).

\section{Dataset e Data Engineering}
Per soddisfare il requisito di progetto relativo alla gestione attiva dei dati, abbiamo evitato l'utilizzo di dataset pre-confezionati. Abbiamo invece sviluppato una procedura di generazione di dati sintetici, necessaria per applicare tecniche di apprendimento supervisionato in un contesto di gioco.

\subsection{Metodologia di Generazione e Etichettatura}
Non disponendo di un esperto umano per etichettare migliaia di configurazioni, abbiamo utilizzato un approccio algoritmico.
La pipeline di generazione dei dati, implementata nello script \texttt{generate\_dataset.py}, segue questi passi:

\begin{enumerate}
    \item \textbf{Campionamento dello Spazio degli Stati:} Vengono simulate partite casuali fino a raggiungere un numero di mosse variabile $N \in [4, 24]$. Questo permette di ottenere configurazioni di "metà partita" variegate e realistiche, evitando che la rete apprenda solo le fasi iniziali.
    
    \item \textbf{Etichettatura tramite Oracolo (Minimax):} Per determinare il target $y$ (chi sta vincendo?), utilizziamo l'algoritmo Minimax come "Oracolo". L'algoritmo esplora l'albero di gioco a profondità limitata e calcola un punteggio euristico per la configurazione corrente.
    
    \item \textbf{Discretizzazione:} Il punteggio numerico restituito dal Minimax viene convertito in classi discrete per la classificazione:
    \begin{itemize}
        \item Classe 1: Vittoria sicura/probabile del Giocatore 1.
        \item Classe -1: Vittoria sicura/probabile del Giocatore 2 (AI).
        \item Classe 0: Situazione di pareggio o incerta.
    \end{itemize}
\end{enumerate}
\noindent
Questa metodologia ci ha permesso di creare un dataset bilanciato e rappresentativo delle diverse fasi del gioco, fornendo alla rete neurale un'ampia varietà di esempi per l'addestramento.

\subsection{Struttura del File Dati (CSV)}
Il dataset costruito (\texttt{connect4\_dataset\_hq.csv}) segue una struttura tabellare standard ed è composto da un totale di \textbf{43 colonne} per ogni riga (campione).\\\\
\noindent
Le prime 42 colonne rappresentano lo stato completo della scacchiera. Poiché Forza 4 si gioca su una griglia bidimensionale di dimensioni $R=6$ (righe) e $C=7$ (colonne), il numero totale di celle è 42. \\\\
\noindent
Per rendere i dati compatibili con l'input del Multi-Layer Perceptron, la matrice 2D viene sottoposta a un processo di \textbf{linearizzazione} (flattening), trasformandola in un vettore 1D.

\begin{itemize}
    \item \textbf{Input ($X$):} Le colonne sono etichettate da \texttt{pos\_0} a \texttt{pos\_41}. L'indice $i$ della feature \texttt{pos\_i} corrisponde alla cella della scacchiera alle coordinate $(r, c)$ secondo la formula $i = r \times 7 + c$. Ogni cella contiene un valore intero discreto:
    \begin{itemize}
        \item $0$: Cella Vuota.
        \item $1$: Pedina del Giocatore 1.
        \item $-1$: Pedina del Giocatore 2.
    \end{itemize}
    
    \item \textbf{Target ($y$):} L'ultima colonna, denominata \texttt{winner}, costituisce l'etichetta di classe per l'addestramento supervisionato. Questa colonna non contiene una mossa, ma la valutazione strategica fornita dall'Oracolo ($1, -1, 0$).
\end{itemize}
\noindent
Per la definizione dello schema dei dati, abbiamo preso a riferimento il \textit{Connect-4 Game Dataset}\cite{2}. Sebbene i dati siano stati generati internamente per garantire il controllo sulla distribuzione, abbiamo deciso di mantenere la medesima struttura logica del dataset di riferimento. \\\\
\noindent
Questa scelta garantisce l'interoperabilità e permette il confronto con standard consolidati nel dominio del Machine Learning applicato ai giochi da tavolo.

\section{Soluzione del problema}
Il progetto TWAI implementa e confronta due approcci radicalmente diversi per la risoluzione del problema: un approccio simbolico basato sulla ricerca (Pipeline A) e un approccio connessionista basato sull'apprendimento (Pipeline B).

\subsection{Pipeline A: Minimax con Alpha-Beta Pruning}
Questa pipeline rappresenta l'approccio classico dell'Intelligenza Artificiale simbolica. L'agente non "impara", ma "ragiona" esplorando le conseguenze future delle azioni.

\subsubsection{Cenni Teorici}
L'algoritmo Minimax è una procedura ricorsiva per la scelta della mossa ottimale in giochi avversari. Dato uno stato $S$:
\begin{itemize}
    \item \textbf{MAX (Agente):} Cerca di massimizzare il valore della funzione di utilità.
    \item \textbf{MIN (Avversario):} Cerca di minimizzare il valore della funzione di utilità.
\end{itemize}
\noindent
Il valore di un nodo è determinato ricorsivamente dai valori dei suoi figli. Per ottimizzare la ricerca, abbiamo implementato la potatura \textbf{Alpha-Beta Pruning}.\\\\
\noindent
Questa tecnica mantiene due parametri, $\alpha$ (miglior valore per MAX) e $\beta$ (miglior valore per MIN). Se durante l'esplorazione si trova una mossa che peggiora la situazione rispetto a quanto già garantito dai rami precedenti, l'intero ramo viene tagliato (pruned). Ciò riduce la complessità temporale nel caso medio da $O(b^d)$ a $O(b^{d/2})$, permettendo esplorazioni più profonde a parità di tempo.

\subsubsection{Ottimizzazione: Ordinamento delle Mosse (Move Ordering)}
L'efficacia della potatura Alpha-Beta dipende fortemente dall'ordine in cui le mosse vengono valutate. Nel caso peggiore (in cui le mosse migliori vengono esaminate per ultime), l'algoritmo degenera in un Minimax puro, rendendo inutile l'ottimizzazione.\\\\
\noindent
Per mitigare questo rischio, abbiamo implementato un'euristica di ordinamento statico specifica per il dominio di Forza 4.
Poiché le colonne centrali della griglia offrono un numero maggiore di potenziali allineamenti (orizzontali, verticali e diagonali) rispetto a quelle laterali, è statisticamente più probabile che la mossa ottimale si trovi al centro.\\\\
\noindent
Invece di iterare le colonne in ordine sequenziale (da 0 a 6), l'algoritmo esplora lo spazio delle azioni partendo dal centro verso l'esterno, seguendo l'ordine di priorità:
\[ [3, 2, 4, 1, 5, 0, 6] \]
Questa semplice accortezza permette all'algoritmo di trovare rapidamente un valore di $\alpha$ elevato (una mossa forte), massimizzando il numero di rami tagliati (pruning) e riducendo significativamente il tempo di calcolo effettivo.

\subsection{Pipeline B: Apprendimento Automatico (MLP)}
La seconda soluzione applica i concetti di apprendimento supervisionato discussi nel corso, utilizzando una rete neurale per approssimare la funzione di valutazione.

\subsubsection{Architettura della Rete Neurale}
Abbiamo progettato un Multi-Layer Perceptron (MLP), una rete neurale feed-forward costituita da strati densi completamente connessi.
La scelta di questa architettura non è casuale ma necessaria. I percettroni a singolo strato sono limitati alla risoluzione di problemi linearmente separabili\cite{3}.\\\\
\noindent
Poiché la valutazione di una scacchiera di Forza 4 presenta confini decisionali altamente non lineari (una singola pedina può cambiare l'esito da sconfitta a vittoria), è indispensabile l'introduzione di strati nascosti (\textit{hidden layers}).\\\\
\noindent
La topologia implementata sfrutta la capacità dell'MLP di agire come approssimatore universale di funzioni ed è così composta:

\begin{enumerate}
    \item \textbf{Input Layer (42 Neuroni):} Riceve il vettore linearizzato della scacchiera.
    \item \textbf{Hidden Layers (64 e 32 Neuroni):} Due strati densi incaricati di estrarre feature latenti dalle posizioni delle pedine.
    \item \textbf{Activation Function (ReLU):} Utilizziamo la \textit{Rectified Linear Unit} ($f(x) = \max(0, x)$) per gli strati nascosti. Questa funzione è preferita alla sigmoide per le reti profonde in quanto mitiga il problema del \textit{vanishing gradient} e favorisce una convergenza più rapida.
    \item \textbf{Output Layer (Classificazione):} Restituisce la predizione sulla classe vincente.
\end{enumerate}

\subsubsection{Addestramento (Backpropagation)}
Il modello è stato addestrato utilizzando l'algoritmo di \textbf{Backpropagation} (propagazione all'indietro dell'errore).
L'obiettivo dell'addestramento è muoversi sulla "superficie dell'errore" per trovare il minimo globale, ovvero la configurazione di pesi che minimizza la differenza tra l'output della rete e il target desiderato. \cite{4} \\\\
\noindent
Il ciclo di apprendimento per ogni epoca segue tre passi fondamentali:

\begin{enumerate}
    \item \textbf{Forward Pass:} Il vettore di input (la scacchiera) attraversa la rete strato per strato fino a produrre un output finale.
    
    \item \textbf{Calcolo dell'Errore:} Viene calcolata la differenza tra l'output prodotto dalla rete e il target corretto fornito dall'Oracolo Minimax.
    
    \item \textbf{Backward Pass:} L'errore viene propagato all'indietro dall'output verso l'input. I pesi delle connessioni vengono aggiornati proporzionalmente al gradiente negativo dell'errore (Discesa del Gradiente), permettendo alla rete di correggere progressivamente le proprie predizioni.
\end{enumerate}
\noindent
Questo processo iterativo consente al modello di apprendere la strategia di gioco codificata nei dati sintetici, riducendo l'errore di classificazione nel tempo.

\section{Valutazione Sperimentale}
L'obiettivo di questa fase non è solo misurare la "bravura" dell'IA, ma valutare se il sistema è utilizzabile in un contesto reale. Abbiamo analizzato due aspetti chiave: la capacità di generalizzazione (per la Rete Neurale) e la reattività temporale (per il Minimax).
I test sono stati eseguiti su una macchina locale, simulando le condizioni tipiche di un utente medio.

\subsection{Analisi Rete Neurale (MLP)}
Per l'approccio basato su Machine Learning, ci siamo chiesti: "La rete sta davvero imparando o sta solo memorizzando?". Per rispondere, abbiamo monitorato due indicatori grafici.

\subsubsection{Curva di Apprendimento (Loss)}
Il grafico della \textit{Loss Function} rappresenta l'errore commesso dalla rete durante l'addestramento.

\begin{figure}[H]
    \centering
        \includegraphics[width=1\linewidth]{images/training_loss_curve.png}
        \caption{Curva di convergenza della Loss.}
\end{figure}
\noindent
Come si nota dalla curva, l'errore crolla drasticamente nelle prime epoche: questo indica che la rete apprende molto velocemente le regole base (es. "mettere 4 gettoni in fila vince"). \\\\
\noindent
Successivamente, la curva si appiattisce (fase di \textit{plateau}), indicando che il modello sta affinando la sua strategia per le situazioni più sottili. Il fatto che la curva non risalga suggerisce l'assenza di \textit{overfitting} (la rete non sta "imparando a memoria").

\subsubsection{Analisi degli Errori (Matrice di Confusione)}
L'accuratezza globale dell'84\% è un ottimo risultato, ma dove sbaglia l'IA? La matrice di confusione ci aiuta a capirlo.

\begin{figure}[H]
    \centering
        \includegraphics[width=0.75\linewidth]{images/confusion_matrix.png}
        \caption{Matrice di Confusione (Modello preliminare, dataset ridotto).}
\end{figure}
\noindent
La diagonale principale (i valori corretti) è molto marcata. La rete riconosce quasi perfettamente le vittorie e le sconfitte imminenti.\\\\
\noindent
 La maggior parte degli errori si concentra sulla classe "Pareggio/Incerto". Questo è comprensibile: anche per un umano è difficile prevedere un pareggio con 20 mosse di anticipo. La rete tende a essere "ottimista" o "pessimista" in situazioni di stallo, un comportamento tipico degli approcci probabilistici.

\subsubsection{Impatto della Dimensione del Dataset}
Un fattore determinante per le prestazioni del modello MLP è la quantità di dati forniti in ingresso. Le reti neurali sono algoritmi "data-hungry": la loro capacità di generalizzare cresce all'aumentare della varietà degli esempi visti.\\\\
\noindent
Abbiamo condotto un'analisi di sensibilità addestrando la stessa architettura di rete su dataset di dimensioni crescenti, mantenendo inalterati gli iperparametri (learning rate, nodi hidden).

\begin{table}[H]
\centering
\caption{Relazione tra dimensione del Dataset e Accuratezza}
\label{tab:dataset_accuracy}
\begin{tabular}{ccc}
\toprule
\textbf{N. Campioni} & \textbf{Accuratezza (Test Set)} & \textbf{Delta Miglioramento} \\
\midrule
2000 & 67.12\% & - \\
10.000 & 71.30\% & +4.18\% \\
20.000 & 75.33\% & +4.03\% \\
30.000 & 79.88\% & +4.55\% \\
50.000 & 82.32\% & +2.44\% \\
\textbf{100.000} & \textbf{85.95\%} & \textbf{+3.63\%} \\
\bottomrule
\end{tabular}
\end{table}
\noindent
\noindent
Come evidenziato in Tabella \ref{tab:dataset_accuracy}, l'accuratezza scala in modo significativo con la quantità di dati. È interessante notare come il passaggio da 50.000 a 100.000 campioni abbia portato un ulteriore incremento del 3.63\%, portando il modello a sfiorare l'86\% di precisione totale. \\\\
\noindent
Questo risultato conferma che per giochi posizionali complessi come Forza 4, la varietà delle situazioni di gioco (specialmente nel "mid-game") è fondamentale. Abbiamo pertanto selezionato il modello addestrato su \textbf{100.000 campioni} come versione definitiva per il deployment nell'applicazione finale.

\begin{figure}[H]
    \centering
     \includegraphics[width=0.75\linewidth]{images/confusion_matrix_100k.png} 
    \caption{Matrice di Confusione (Modello finale su 100k campioni).}
\end{figure}

\subsubsection{Dettaglio Metriche (Precision, Recall, F1)}
Il dataset totale di 100.000 campioni è stato partizionato seguendo uno split 80/20 standard: il modello è stato addestrato su 80.000 campioni (Training Set) e successivamente valutato su 20.000 campioni inediti (Test Set). \\\\
\noindent
Le metriche riportate di seguito (Precision, Recall, F1) si riferiscono esclusivamente alle performance ottenute su quest'ultimo set, per garantire una stima imparziale delle capacità di generalizzazione dell'agente.

\begin{table}[H]
\centering
\caption{Report di Classificazione sul Test Set}
\label{tab:classification_report}
\begin{tabular}{lcccc}
\toprule
\textbf{Classe} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\
\midrule
AI Wins (-1)    & 0.78 & 0.74 & 0.76 & 3946 \\
Draw / Uncertain (0)        & 0.90 & 0.91 & 0.90 & 10110 \\
Player Wins (1) & 0.82 & 0.85 & 0.83 & 5944 \\
\midrule
\textbf{Weighted Avg} & \textbf{0.86} & \textbf{0.86} & \textbf{0.86} & \textbf{20000} \\
\bottomrule
\end{tabular}
\end{table}

\noindent
Dai dati in Tabella \ref{tab:classification_report} emergono considerazioni interessanti sulla strategia appresa dalla rete:

\begin{itemize}
    \item \textbf{Solidità sui Pareggi (Classe 0):} La rete ha un F1-Score molto alto (0.90) su questa classe. Poiché i pareggi o le situazioni di stallo sono la maggioranza nel dataset (circa il 50\% del supporto), il modello è diventato molto abile nel riconoscere quando "non sta succedendo nulla di decisivo".
    
    \item \textbf{Prudenza dell'IA (Classe -1):} La \textit{Recall} sulle vittorie dell'IA è la più bassa (0.74). Questo significa che il modello perde l'occasione di identificare alcune delle sue vittorie (Falsi Negativi), probabilmente a causa di una strategia conservativa appresa per minimizzare le sconfitte piuttosto che massimizzare le vittorie rischiose.
    
    \item \textbf{Riconoscimento Minaccia (Classe 1):} La rete è più brava a riconoscere quando sta vincendo l'avversario (Recall 0.85) rispetto a quando sta vincendo lei stessa. Questo è un comportamento desiderabile in un gioco difensivo: è meglio sovrastimare il pericolo che ignorarlo.
\end{itemize}
\noindent

\subsection{Benchmark Minimax (Reattività)}
Per l'algoritmo Minimax, la precisione è garantita dalla matematica. Il vero nemico è il tempo di esecuzione, che cresce esponenzialmente con la profondità di ricerca.
In una prima fase, abbiamo misurato le prestazioni dell'algoritmo applicando la sola potatura Alpha-Beta con un ordine di visita delle colonne standard (sequenziale da 0 a 6).

\begin{table}[H]
\centering
\caption{Tempi di esecuzione rilevati per Minimax con Alpha-Beta Pruning (Standard)}
\label{tab:real_benchmark_standard}
\begin{tabular}{cccc}
\toprule
\textbf{Profondità ($d$)} & \textbf{Tempo Medio (s)} & \textbf{Nodi Stimati} \\
\midrule
1 & 0.0002 & $\sim 7$ \\
2 & 0.0006 & $\sim 49$ \\
3 & 0.0034 & $\sim 340$ \\
4 & 0.0174 & $\sim 2,400$  \\
5 & 0.0757 & $\sim 16,800$ \\
6 & \textbf{0.2858} & $\sim 117,000$ \\
\bottomrule
\end{tabular}
\end{table}

\noindent
Dai dati in Tabella \ref{tab:real_benchmark_standard} emerge chiaramente la natura esponenziale del problema: passando da profondità 5 a 6, il tempo quadruplica (da 0.07s a 0.28s). Sebbene 0.28s sia un tempo accettabile, lascia poco margine per ulteriori approfondimenti.\\\\
\noindent
Per mitigare questo effetto, abbiamo introdotto l'ottimizzazione di Ordinamento delle Mosse, forzando l'algoritmo a valutare prima le colonne centrali (più promettenti). Questo aumenta la probabilità di effettuare "tagli" (pruning) anticipati nell'albero di ricerca. I risultati migliorati sono riportati di seguito:

\begin{table}[H]
\centering
\caption{Tempi di esecuzione rilevati per Minimax con Alpha-Beta Pruning + Ordinamento delle Mosse}
\label{tab:real_benchmark_ordered}
\begin{tabular}{cccc}
\toprule
\textbf{Profondità ($d$)} & \textbf{Tempo Medio (s)} & \textbf{Nodi Stimati} \\
\midrule
1 & 0.0005 & $\sim 7$ \\
2 & 0.0009 & $\sim 49$ \\
3 & 0.0051 & $\sim 340$ \\
4 & 0.0111 & $\sim 2,400$  \\
5 & 0.0310 & $\sim 16,800$ \\
6 & \textbf{0.1328} & $\sim 117,000$ \\
7 & 0.2956 & $\sim 820,000$ \\
\bottomrule
\end{tabular}
\end{table}
\noindent
\subsubsection*{Analisi del miglioramento}

Il confronto tra le due configurazioni evidenzia drasticamente l'efficacia dell'ottimizzazione. A parità di profondità $d=6$, il tempo di calcolo è stato abbattuto di oltre il \textbf{50\%}, scendendo da \textbf{0.2858s} a \textbf{0.1328s}. \\\\
Un dato particolarmente interessante emerge osservando il passo successivo: spingendo l'algoritmo ottimizzato a profondità $d=7$, il tempo di esecuzione risale a \textbf{0.2956s}. Questo valore è quasi sovrapponibile a quello impiegato dall'algoritmo non ottimizzato per completare la ricerca a $d=6$ (0.2858s). Pertanto, spingersi a $d=7$ rimane sconsigliato, poiché significherebbe vanificare il margine di guadagno ottenuto, riportando il sistema ai limiti della latenza percepibile. \\\\
Abbiamo identificato in \textbf{$d=6$} la configurazione ideale per il sistema finale. Con un tempo di risposta ora ben al di sotto della soglia di percezione (0.13s), l'IA offre un'esperienza di gioco fluida e immediata (``Real-time''), pur mantenendo una profondità di analisi strategica elevata.
\subsection{Trade-off}
Il confronto finale evidenzia due filosofie opposte:
\begin{enumerate}
    \item \textbf{Minimax:} È infallibile entro il suo orizzonte ($d$), ma consuma risorse CPU durante la partita. È ideale quando la precisione tattica è prioritaria.
    \item \textbf{MLP (Rete Neurale):} Una volta addestrata, ha un tempo di inferenza costante e bassissimo ($O(1)$), indipendentemente dalla complessità della scacchiera. Tuttavia, essendo un approccio probabilistico, può commettere errori "umani" in configurazioni mai viste.
\end{enumerate}

\section{Conclusioni}
Il progetto T.W.A.I. ha raggiunto l'obiettivo di confrontare due paradigmi fondamentali dell'Intelligenza Artificiale applicati a un dominio a informazione perfetta. \\
I risultati confermano che, mentre l'approccio simbolico (Minimax) rimane insuperabile in termini di correttezza formale, l'approccio connessionista (MLP) offre un vantaggio decisivo in termini di scalabilità temporale, al costo di una marginale perdita di precisione.\\\\
\noindent
L'esperienza ha inoltre evidenziato l'importanza cruciale della fase di Data Engineering: la qualità del dataset generato si è rivelata più determinante dell'architettura della rete stessa per il successo dell'apprendimento.\\\\
\noindent
In conclusione, T.W.A.I. dimostra che, sebbene "Tris Was Already Invented", c'è ancora spazio per l'innovazione e la sperimentazione nell'ambito dei giochi da tavolo, offrendo spunti interessanti per futuri sviluppi in IA. 

\newpage    
\printbibliography

\end{document}
