\documentclass[12pt]{article}

% --- Lingua e Codifica ---
\usepackage[italian,english]{babel}

% --- Matematica ---
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{bm}

% --- Colori e Grafica ---
\usepackage[table,xcdraw,svgnames]{xcolor} 
\usepackage{graphicx} % Required for inserting images & resizing

% --- Tabelle ---
\usepackage{array}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{adjustbox}

% --- Formattazione e Layout ---
\usepackage[italian]{minitoc}
\usepackage{fancybox}
\usepackage{fancyhdr}
\usepackage{lscape}
\usepackage{placeins}
\usepackage{float}
\usepackage{caption}
\usepackage{soul}

% --- Utilità e Codice ---
\usepackage{verbatim}
\usepackage{url}
\usepackage{listings}
\usepackage{makeidx}
\usepackage{comment}
\usepackage{animate}

% --- Bibliografia ---
\usepackage{biblatex}
\addbibresource{sample.bib}

% --- Collegamenti (Caricare per ultimo) ---

%\titleformat{\chapter}{\normalfont\huge}{\textbf\thechapter.}{20pt}%{\huge\textbf}

%inizio documento
\begin{document}
\selectlanguage{italian}

%inizio copertina
\begin{titlepage}
\begin{center}
    \begin{figure}
        \includegraphics[width=3.0cm, height=3.0cm]{images/unisa.png}
        \centering
    \end{figure}
    {\Large Università degli Studi di Salerno}\\[0.2truecm]
    {\large Dipartimento di Informatica\\Corso di Laurea Triennale in Informatica}\\
    \hrulefill
    \vfill
    {\large Fondamenti di Intelligenza Artificiale (FIA)}\\[0.2truecm]
    %{\large Project Proposal}\\[0.2truecm]
    %{\Large Informatica}\\
    \vfill
    {\Huge {\bf T.W.A.I.}}\\
    \vspace{0.3cm}
    {\Large \textit{(Tris Was Already Invented)}}\\
    
    \vfill\vfill
    
    
    {\bf Docente} \hfill {\bf Studenti}\ \hfill  {\bf Matricola}\  \\
    Prof.  Fabio Palomba \hfill Tozza Gennaro Carmine \hfill 0512120382 \\
    \hfill \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ Valva Lorenzo \hfill 0512119639 \\
    \vfill
    [\url{https://github.com/gennarocarmine/fia-project.git}]
    \vfill
    \hrulefill 
    \begin{center} Anno Accademico 2025-2026 \end{center}
    
\end{center}
\end{titlepage}
%fine copertina

\tableofcontents
\newpage

\begin{abstract}
\noindent
Il presente elaborato documenta lo sviluppo di T.W.A.I. (Tris Was Already Invented), un sistema di Intelligenza Artificiale applicato al gioco Forza 4. L'obiettivo del progetto è confrontare due paradigmi fondamentali dell'IA: l'approccio simbolico, implementato tramite l'algoritmo Minimax con ottimizzazione Alpha-Beta Pruning, e l'approccio connessionista, realizzato mediante una Rete Neurale MLP (Multi-Layer Perceptron).
Particolare attenzione è stata posta alla generazione autonoma del dataset di training, superando i limiti delle simulazioni casuali. I risultati sperimentali dimostrano come l'approccio neurale, sebbene probabilisticamente imperfetto (86\% di accuratezza), offra tempi di risposta costanti ($O(1)$), rappresentando un'alternativa valida e scalabile rispetto alla complessità esponenziale dell'algoritmo di ricerca.
\end{abstract}

\newpage

\section{Introduzione}
Il periodo festivo porta con sé tradizioni imprescindibili: i panettoni, le calze e le discussioni con i parenti che, pur lamentandosi dell'invadenza dell'Intelligenza Artificiale, passano il tempo a guardare video di gattini che ballano sui social. Ma porta anche le immancabili giocate a carte. 
È stato proprio durante l'ennesima partita persa a \textit{Sette e Mezzo} che è sorta la domanda fatidica: ``E se al nostro posto giocasse un'IA?''.\\\\
\noindent
L'idea embrionale del progetto, inizialmente battezzata con il nome in codice \textbf{(IA)'M FINE} (un gioco di parole sulla dichiarazione ``sto bene'' usata per non ricevere altre carte), mirava a creare un agente imbattibile in questo gioco tradizionale.\\\\
\noindent
Tuttavia, durante la fase di analisi preliminare, è emersa una criticità strutturale. A differenza di giochi come il Blackjack, dove il banco espone parzialmente il proprio stato (una carta scoperta e una coperta), nel Sette e Mezzo l'interazione inizia con un livello di informazione imperfetta molto marcato, dato che la carta iniziale è privata e coperta.
Questa caratteristica sposta il problema dal ragionamento strategico puro alla gestione della probabilità e del rischio (stocasticità), rendendo difficile un confronto diretto tra l'efficacia di algoritmi deterministici come il Minimax.

\subsubsection*{Cambio di Rotta}
Cercando un'alternativa deterministica (a informazione perfetta), il pensiero è corso subito al gioco più classico: il Tris (Tic-Tac-Toe). Ma anche qui sorgeva un problema opposto: il Tris è un gioco banale, con uno spazio degli stati minuscolo e una strategia ottimale che porta inevitabilmente al pareggio.\\\
\noindent
Da qui nasce il nome definitivo del progetto: \textbf{T.W.A.I.} (\textit{Tris Was Already Invented}).\\\\
\noindent
Abbiamo quindi scelto il ``fratello maggiore'' del Tris: \textbf{Forza 4} (Connect-4). In questo contesto, l'intero stato del gioco è visibile a entrambi i contendenti, eliminando il fattore fortuna e permettendo di concentrare l'analisi esclusivamente sulle capacità computazionali e strategiche dell'Intelligenza Artificiale.

\section{Definizione del problema} 
Connect Four (noto anche come Connect 4, Four Up, Plot Four, Find Four, Captain's Mistress, Four in a Row, Drop Four, e in Unione Sovietica, Gravitrips) è un gioco in cui i giocatori scelgono un colore e poi si alternano lasciando cadere gettoni colorati in una griglia a sei file, sette colonne verticalmente sospesa. \\
I pezzi cadono direttamente, occupando lo spazio più basso disponibile all'interno della colonna. L'obiettivo del gioco è quello di essere il primo a formare una linea orizzontale, verticale o diagonale di quattro dei propri gettoni. \cite{5}

\begin{figure}[H]
    \centering
        \includegraphics[width=0.4\textwidth]{images/connect_four.png}
    \caption{Immagine del gioco Forza 4.}
\end{figure}
\noindent
Il gioco rientra nella categoria dei giochi a somma zero, a informazione perfetta e deterministici. Sebbene le regole siano semplici, la complessità computazionale non è trascurabile.\\\\
\noindent
Nonostante il gioco sia stato "risolto" matematicamente (il primo giocatore ha una strategia vincente se gioca perfettamente), per un agente limitato da risorse computazionali e tempo di risposta (real-time interaction), la ricerca esaustiva è impraticabile.
Il problema richiede quindi l'utilizzo di euristiche o approssimatori di funzione per valutare la bontà di una mossa senza dover esplorare l'intero albero di gioco fino alle foglie.

\subsection{Obiettivi} 
L'obiettivo principale del progetto è sviluppare e confrontare due diversi approcci per la risoluzione del gioco Forza 4: uno basato sulla teoria della ricerca nello spazio degli stati e uno basato sull'apprendimento automatico supervisionato. Il sistema non utilizzerà dataset precostruiti, ma genererà autonomamente i dati necesaari per l'addestramento. Nello specifico, gli obiettivi sono:
\begin{itemize}
    \item \textbf{Generare un Dataset:} Creare un dataset di training personalizzato che superi i limiti delle simulazioni puramente casuali (Random vs Random). Il dataset verrà generato registrando partite miste:
     \begin{itemize}
        \item Bot Random vs Bot Random: Per esplorare lo spazio degli stati in modo ampio.
        \item Minimax vs Random: Per insegnare alla rete neurale pattern strategici, mosse di blocco e sequenze vincenti, garantendo dati di qualità superiore per l'addestramento.
     \end{itemize}
     L'obiettivo è ottenere un dataset ricco e variegato che consenta alla rete neurale di apprendere strategie efficaci.
    \item \textbf{Implementare la pipeline di Ricerca(Minimax):} Sviluppare un agente basato sull'algoritmo Minimax con Alpha-Beta Pruning. Questo modulo avrà il duplice scopo di:
     \begin{itemize}
        \item Fornire un avversario algoritmico "forte" in grado di calcolare la mossa ottimale esplorando l'albero di gioco.
        \item Agire come "oracolo" per la generazione dei dati di training.
     \end{itemize}
    \item \textbf{Implementare la pipeline di Apprendimento (MLP):} Sviluppare e addestrare una rete neurale (MLP) utilizzando il dataset generato. L'obiettivo è ottenere un modello in grado di predire la mossa migliore istantaneamente (approssimando la funzione di valutazione), imitando la logica del Minimax ma con tempi di risposta drasticamente inferiori.
    \item \textbf{Sviluppare un'interfaccia interattiva:} Creare un interfaccia grafica che permetta all'utente di sfidare entrambe le IA (Minmax e MLP).
\end{itemize}

\subsection{Specifica PEAS} 
La descrizione formale dell'ambiente operativo dell'agente è definita secondo il modello PEAS (\textit{Performance, Environment, Actuators, Sensors}):

\begin{itemize}
    \item \textbf{Performance (Misure di Prestazione):}
    \begin{itemize}
        \item Efficienza: Numero di mosse minimo per raggiungere la vittoria.
        \item Correttezza: Evitare mosse non valide (es. inserire in una colonna piena).
        \item Velocità di risposta: Tempo impiegato per calcolare la mossa.
        \item Tasso di vittoria: Percentuale di partite vinte contro vari tipi di avversari.
    \end{itemize}
    
    \item \textbf{Environment (Ambiente):}
    \begin{itemize}
        \item Griglia di gioco: Una matrice 6 X 7.
        \item Avversario: Un essere umano o un'altra IA.
    \end{itemize}
    
    \item \textbf{Actuators (Attuatori/Azioni):}
    \begin{itemize}
        \item Inserimento gettone: l'attuatore sceglie una delle 7 colonne disponibili.
        \item Segnalazione: comunicazione della mossa o dichiarazione di vittoria/resa.
    \end{itemize}
    
    \item \textbf{Sensors (Sensori/Percezioni):}
    \begin{itemize}
        \item Lettore di matrice: Funzione che scansiona lo sttao attuale della scacchiera per sapere dove sono i propri gettoni, quelli dell'avversario e gli spazi vuoti.
        \item Rilevatore di turno: Funziona che indica alla'gente quando è il suo momento di agire.
    \end{itemize}
\end{itemize}

\subsubsection{Caratteristiche dell’ambiente} 
L'ambiente di gioco Forza 4 presenta le seguenti proprietà:
\begin{itemize}
    \item \textbf{Completamente Osservabile:} L'agente vede l'intera griglia di gioco, non ci osno informazioni nascoste.
    \item \textbf{Deterministico:} Lo stato successivo dell'ambiente è completamente determinato dallo stato corrente e dall'azione eseuita dall'agente.
    \item \textbf{Discreto:} L'ambiente fornisce un numero limitato di percezioni e azioni distinte, chiaramente definite.
\end{itemize}

\subsection{Analisi del problema} 
Lo spazio degli stati teorico è costituito da tutte le possibili configurazioni di una griglia $6 \times 7$ con tre possibili valori per cella (Vuoto, Giocatore 1, Giocatore 2), portando a un limite superiore di $3^{42}$ stati. Tuttavia, considerando i vincoli di gravità (le pedine devono poggiare su altre pedine o sul fondo), il numero reale di stati legali è stimato intorno a $4.5 \times 10^{12}$.


\section{Dataset e Data Engineering}
Per soddisfare il requisito di progetto relativo alla gestione attiva dei dati, abbiamo evitato l'utilizzo di dataset pre-confezionati. Abbiamo invece sviluppato una procedura di generazione di dati sintetici, necessaria per applicare tecniche di apprendimento supervisionato in un contesto di gioco.

\subsection{Metodologia di Generazione e Etichettatura}
Non disponendo di un esperto umano per etichettare migliaia di configurazioni, abbiamo utilizzato un approccio algoritmico.
La pipeline di generazione dei dati, implementata nello script \texttt{generate\_dataset.py}, segue questi passi:

\begin{enumerate}
    \item \textbf{Campionamento dello Spazio degli Stati:} Vengono simulate partite casuali fino a raggiungere un numero di mosse variabile $N \in [4, 24]$. Questo permette di ottenere configurazioni di "metà partita" variegate e realistiche, evitando che la rete apprenda solo le fasi iniziali.
    
    \item \textbf{Etichettatura tramite Oracolo (Minimax):} Per determinare il target $y$ (chi sta vincendo?), utilizziamo l'algoritmo Minimax come "Oracolo". L'algoritmo esplora l'albero di gioco a profondità limitata e calcola un punteggio euristico per la configurazione corrente.
    
    \item \textbf{Discretizzazione:} Il punteggio numerico restituito dal Minimax viene convertito in classi discrete per la classificazione:
    \begin{itemize}
        \item Classe 1: Vittoria sicura/probabile del Giocatore 1.
        \item Classe -1: Vittoria sicura/probabile del Giocatore 2 (AI).
        \item Classe 0: Situazione di pareggio o incerta.
    \end{itemize}
\end{enumerate}
\noindent
Questa metodologia ci ha permesso di creare un dataset bilanciato e rappresentativo delle diverse fasi del gioco, fornendo alla rete neurale un'ampia varietà di esempi per l'addestramento.

\subsection{Struttura del File Dati (CSV)}
Il dataset costruito (\texttt{connect4\_dataset\_hq.csv}) segue una struttura tabellare standard ed è composto da un totale di \textbf{43 colonne} per ogni riga (campione).\\\\
\noindent
Le prime 42 colonne rappresentano lo stato completo della scacchiera. Poiché Forza 4 si gioca su una griglia bidimensionale di dimensioni $R=6$ (righe) e $C=7$ (colonne), il numero totale di celle è 42. \\\\
\noindent
Per rendere i dati compatibili con l'input del Multi-Layer Perceptron, la matrice 2D viene sottoposta a un processo di \textbf{linearizzazione} (flattening), trasformandola in un vettore 1D.

\begin{itemize}
    \item \textbf{Input ($X$):} Le colonne sono etichettate da \texttt{pos\_0} a \texttt{pos\_41}. L'indice $i$ della feature \texttt{pos\_i} corrisponde alla cella della scacchiera alle coordinate $(r, c)$ secondo la formula $i = r \times 7 + c$. Ogni cella contiene un valore intero discreto:
    \begin{itemize}
        \item $0$: Cella Vuota.
        \item $1$: Pedina del Giocatore 1.
        \item $-1$: Pedina del Giocatore 2.
    \end{itemize}
    
    \item \textbf{Target ($y$):} L'ultima colonna, denominata \texttt{winner}, costituisce l'etichetta di classe per l'addestramento supervisionato. Questa colonna non contiene una mossa, ma la valutazione strategica fornita dall'Oracolo ($1, -1, 0$).
\end{itemize}
\noindent
Per la definizione dello schema dei dati, abbiamo preso a riferimento il \textit{Connect-4 Game Dataset}\cite{2}. Sebbene i dati siano stati generati internamente per garantire il controllo sulla distribuzione, abbiamo deciso di mantenere la medesima struttura logica del dataset di riferimento. \\\\
\noindent
Questa scelta garantisce l'interoperabilità e permette il confronto con standard consolidati nel dominio del Machine Learning applicato ai giochi da tavolo.

\section{Soluzione del problema}
Il progetto TWAI implementa e confronta due approcci radicalmente diversi per la risoluzione del problema: un approccio simbolico basato sulla ricerca (Pipeline A) e un approccio connessionista basato sull'apprendimento (Pipeline B).

\subsection{Pipeline A: Minimax con Alpha-Beta Pruning}
Questa pipeline rappresenta l'approccio classico dell'Intelligenza Artificiale simbolica. L'agente non "impara", ma "ragiona" esplorando le conseguenze future delle azioni.

\subsubsection{Cenni Teorici}
L'algoritmo Minimax è una procedura ricorsiva per la scelta della mossa ottimale in giochi avversari. Dato uno stato $S$:
\begin{itemize}
    \item \textbf{MAX (Agente):} Cerca di massimizzare il valore della funzione di utilità.
    \item \textbf{MIN (Avversario):} Cerca di minimizzare il valore della funzione di utilità.
\end{itemize}
\noindent
Il valore di un nodo è determinato ricorsivamente dai valori dei suoi figli. Per ottimizzare la ricerca, abbiamo implementato la potatura \textbf{Alpha-Beta Pruning}.\\\\
\noindent
Questa tecnica mantiene due parametri, $\alpha$ (miglior valore per MAX) e $\beta$ (miglior valore per MIN). Se durante l'esplorazione si trova una mossa che peggiora la situazione rispetto a quanto già garantito dai rami precedenti, l'intero ramo viene tagliato (pruned). Ciò riduce la complessità temporale nel caso medio da $O(b^d)$ a $O(b^{d/2})$, permettendo esplorazioni più profonde a parità di tempo.

\subsection{Pipeline B: Apprendimento Automatico (MLP)}
La seconda soluzione applica i concetti di apprendimento supervisionato discussi nel corso, utilizzando una rete neurale per approssimare la funzione di valutazione.

\subsubsection{Architettura della Rete Neurale}
Abbiamo progettato un Multi-Layer Perceptron (MLP), una rete neurale feed-forward costituita da strati densi completamente connessi.
La scelta di questa architettura non è casuale ma necessaria. I percettroni a singolo strato sono limitati alla risoluzione di problemi linearmente separabili\cite{3}.\\\\
\noindent
Poiché la valutazione di una scacchiera di Forza 4 presenta confini decisionali altamente non lineari (una singola pedina può cambiare l'esito da sconfitta a vittoria), è indispensabile l'introduzione di strati nascosti (\textit{hidden layers}).\\\\
\noindent
La topologia implementata sfrutta la capacità dell'MLP di agire come approssimatore universale di funzioni ed è così composta:

\begin{enumerate}
    \item \textbf{Input Layer (42 Neuroni):} Riceve il vettore linearizzato della scacchiera.
    \item \textbf{Hidden Layers (64 e 32 Neuroni):} Due strati densi incaricati di estrarre feature latenti dalle posizioni delle pedine.
    \item \textbf{Activation Function (ReLU):} Utilizziamo la \textit{Rectified Linear Unit} ($f(x) = \max(0, x)$) per gli strati nascosti. Questa funzione è preferita alla sigmoide per le reti profonde in quanto mitiga il problema del \textit{vanishing gradient} e favorisce una convergenza più rapida.
    \item \textbf{Output Layer (Classificazione):} Restituisce la predizione sulla classe vincente.
\end{enumerate}

\subsubsection{Addestramento (Backpropagation)}
Il modello è stato addestrato utilizzando l'algoritmo di \textbf{Backpropagation} (propagazione all'indietro dell'errore).
L'obiettivo dell'addestramento è muoversi sulla "superficie dell'errore" per trovare il minimo globale, ovvero la configurazione di pesi che minimizza la differenza tra l'output della rete e il target desiderato. \cite{4} \\\\
\noindent
Il ciclo di apprendimento per ogni epoca segue tre passi fondamentali:

\begin{enumerate}
    \item \textbf{Forward Pass:} Il vettore di input (la scacchiera) attraversa la rete strato per strato fino a produrre un output finale.
    
    \item \textbf{Calcolo dell'Errore:} Viene calcolata la differenza tra l'output prodotto dalla rete e il target corretto fornito dall'Oracolo Minimax.
    
    \item \textbf{Backward Pass:} L'errore viene propagato all'indietro dall'output verso l'input. I pesi delle connessioni vengono aggiornati proporzionalmente al gradiente negativo dell'errore (Discesa del Gradiente), permettendo alla rete di correggere progressivamente le proprie predizioni.
\end{enumerate}
\noindent
Questo processo iterativo consente al modello di apprendere la strategia di gioco codificata nei dati sintetici, riducendo l'errore di classificazione nel tempo.

\section{Valutazione Sperimentale}
L'obiettivo di questa fase non è solo misurare la "bravura" dell'IA, ma valutare se il sistema è utilizzabile in un contesto reale. Abbiamo analizzato due aspetti chiave: la capacità di generalizzazione (per la Rete Neurale) e la reattività temporale (per il Minimax).
I test sono stati eseguiti su una macchina locale, simulando le condizioni tipiche di un utente medio.

\subsection{Analisi Rete Neurale (MLP)}
Per l'approccio basato su Machine Learning, ci siamo chiesti: "La rete sta davvero imparando o sta solo memorizzando?". Per rispondere, abbiamo monitorato due indicatori grafici.

\subsubsection{Curva di Apprendimento (Loss)}
Il grafico della \textit{Loss Function} rappresenta l'errore commesso dalla rete durante l'addestramento.

\begin{figure}[H]
    \centering
        \includegraphics[width=1\linewidth]{images/training_loss_curve.png}
        \caption{Curva di convergenza della Loss.}
\end{figure}
\noindent
Come si nota dalla curva, l'errore crolla drasticamente nelle prime epoche: questo indica che la rete apprende molto velocemente le regole base (es. "mettere 4 gettoni in fila vince"). \\\\
\noindent
Successivamente, la curva si appiattisce (fase di \textit{plateau}), indicando che il modello sta affinando la sua strategia per le situazioni più sottili. Il fatto che la curva non risalga suggerisce l'assenza di \textit{overfitting} (la rete non sta "imparando a memoria").

\subsubsection{Analisi degli Errori (Matrice di Confusione)}
L'accuratezza globale dell'84\% è un ottimo risultato, ma dove sbaglia l'IA? La matrice di confusione ci aiuta a capirlo.

\begin{figure}[H]
    \centering
        \includegraphics[width=0.75\linewidth]{images/confusion_matrix.png}
        \caption{Matrice di Confusione (Modello preliminare, dataset ridotto).}
\end{figure}
\noindent
La diagonale principale (i valori corretti) è molto marcata. La rete riconosce quasi perfettamente le vittorie e le sconfitte imminenti.\\\\
\noindent
 La maggior parte degli errori si concentra sulla classe "Pareggio/Incerto". Questo è comprensibile: anche per un umano è difficile prevedere un pareggio con 20 mosse di anticipo. La rete tende a essere "ottimista" o "pessimista" in situazioni di stallo, un comportamento tipico degli approcci probabilistici.

\subsubsection{Impatto della Dimensione del Dataset}
Un fattore determinante per le prestazioni del modello MLP è la quantità di dati forniti in ingresso. Le reti neurali sono algoritmi "data-hungry": la loro capacità di generalizzare cresce all'aumentare della varietà degli esempi visti.\\\\
\noindent
Abbiamo condotto un'analisi di sensibilità addestrando la stessa architettura di rete su dataset di dimensioni crescenti, mantenendo inalterati gli iperparametri (learning rate, nodi hidden).

\begin{table}[H]
\centering
\caption{Relazione tra dimensione del Dataset e Accuratezza}
\label{tab:dataset_accuracy}
\begin{tabular}{ccc}
\toprule
\textbf{N. Campioni} & \textbf{Accuratezza (Test Set)} & \textbf{Delta Miglioramento} \\
\midrule
2000 & 67.12\% & - \\
10.000 & 71.30\% & +4.18\% \\
20.000 & 75.33\% & +4.03\% \\
30.000 & 79.88\% & +4.55\% \\
50.000 & 82.32\% & +2.44\% \\
\textbf{100.000} & \textbf{85.95\%} & \textbf{+3.63\%} \\
\bottomrule
\end{tabular}
\end{table}
\noindent
\noindent
Come evidenziato in Tabella \ref{tab:dataset_accuracy}, l'accuratezza scala in modo significativo con la quantità di dati. È interessante notare come il passaggio da 50.000 a 100.000 campioni abbia portato un ulteriore incremento del 3.63\%, portando il modello a sfiorare l'86\% di precisione totale. \\\\
\noindent
Questo risultato conferma che per giochi posizionali complessi come Forza 4, la varietà delle situazioni di gioco (specialmente nel "mid-game") è fondamentale. Abbiamo pertanto selezionato il modello addestrato su \textbf{100.000 campioni} come versione definitiva per il deployment nell'applicazione finale.

\begin{figure}[H]
    \centering
     \includegraphics[width=0.75\linewidth]{images/confusion_matrix_100k.png} 
    \caption{Matrice di Confusione (Modello finale su 100k campioni).}
\end{figure}

\subsection{Benchmark Minimax (Reattività)}
Per l'algoritmo Minimax, la precisione è garantita dalla matematica. Il vero nemico è il tempo. Abbiamo misurato quanto impiega l'IA a "pensare" a diverse profondità ($d$).

\begin{table}[H]
\centering
\caption{Tempi di esecuzione rilevati e Valutazione UX}
\label{tab:real_benchmark}
\begin{tabular}{cccc}
\toprule
\textbf{Profondità ($d$)} & \textbf{Tempo Medio (s)} & \textbf{Nodi Stimati} & \textbf{Esperienza Utente} \\
\midrule
1 & 0.0002 & $\sim 7$ & Istantanea \\
2 & 0.0006 & $\sim 49$ & Istantanea \\
3 & 0.0034 & $\sim 340$ & Impercettibile \\
4 & 0.0174 & $\sim 2,400$ & Molto Fluida \\
5 & 0.0757 & $\sim 16,800$ & Fluida \\
6 & \textbf{0.2858} & $\sim 117,000$ & \textbf{Ottima (Real-time)} \\
\bottomrule
\end{tabular}
\end{table}
\noindent
Dai dati emerge chiaramente la natura esponenziale del problema. Passando da profondità 5 a 6, il tempo quadruplica (da 0.07s a 0.28s). Questo accade perché per ogni livello aggiuntivo, l'algoritmo deve esplorare circa 7 nuove varianti per ogni mossa precedente.\\\\
\noindent
Abbiamo identificato in \textbf{$d=6$} la configurazione ideale. Con un tempo di risposta inferiore a 0.3 secondi, l'IA risulta immediata per l'utente, pur calcolando oltre 100.000 possibili futuri. Spingersi a $d=7$ porterebbe i tempi sopra i 2 secondi, rompendo la fluidità del gioco senza garantire un vantaggio strategico percepibile da un giocatore medio.

\subsection{Trade-off}
Il confronto finale evidenzia due filosofie opposte:
\begin{enumerate}
    \item \textbf{Minimax:} È infallibile entro il suo orizzonte ($d$), ma consuma risorse CPU durante la partita. È ideale quando la precisione tattica è prioritaria.
    \item \textbf{MLP (Rete Neurale):} Una volta addestrata, ha un tempo di inferenza costante e bassissimo ($O(1)$), indipendentemente dalla complessità della scacchiera. Tuttavia, essendo un approccio probabilistico, può commettere errori "umani" in configurazioni mai viste.
\end{enumerate}

\section{Conclusioni}
Il progetto T.W.A.I. ha raggiunto l'obiettivo di confrontare due paradigmi fondamentali dell'Intelligenza Artificiale applicati a un dominio a informazione perfetta. \\
I risultati confermano che, mentre l'approccio simbolico (Minimax) rimane insuperabile in termini di correttezza formale, l'approccio connessionista (MLP) offre un vantaggio decisivo in termini di scalabilità temporale, al costo di una marginale perdita di precisione.\\\\
\noindent
L'esperienza ha inoltre evidenziato l'importanza cruciale della fase di Data Engineering: la qualità del dataset generato si è rivelata più determinante dell'architettura della rete stessa per il successo dell'apprendimento.\\\\
\noindent
In conclusione, T.W.A.I. dimostra che, sebbene "Tris Was Already Invented", c'è ancora spazio per l'innovazione e la sperimentazione nell'ambito dei giochi da tavolo, offrendo spunti interessanti per futuri sviluppi in IA. 

\newpage    
\printbibliography

\end{document}
